{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8772fc8a-65dc-4813-a440-f1ed0f65a563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.48.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440cb064-dc02-4742-a4a7-e298609061c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ce015e-625d-4c88-92f6-dd330697a8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4700f8e5-4095-4900-a7e5-1a618d2811bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003001689910888672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c2366a19834052925f1e9966077e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Source: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-1M\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c7f6b2-d4e3-4878-a893-6b78ea5861a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843463ed-c163-4703-81b1-92eab9904561",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbf6de38-cdf6-43b8-8f3b-c56c0e1e7961",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6890d02-b772-4c71-9ede-9fe010689e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! A **large language model** (LLM) is an advanced type of artificial intelligence (AI) model designed to understand and generate human-like text based on the input it receives. These models are typically trained on vast amounts of text data from the internet, books, websites, and other sources, allowing them to learn patterns, structures, and nuances in language.\n",
      "\n",
      "Key characteristics of LLMs include:\n",
      "\n",
      "1. **Scalability**: They are often composed of billions or even hundreds of billions of parameters, which enable them to capture complex relationships in language.\n",
      "   \n",
      "2. **Context Understanding**: LLMs can generate coherent and contextually appropriate responses, even when given ambiguous or incomplete prompts.\n",
      "\n",
      "3. **Versatility**: They can be fine-tuned for specific tasks, such as translation, summarization, question answering, or creative writing, depending on the application.\n",
      "\n",
      "4. **Continuous Learning**: As more data becomes available, these models can be updated to improve their performance over time.\n",
      "\n",
      "Popular examples of large language models include **GPT-3** (from OpenAI), **T5** (by Google), and **Baichuan** (developed by Alibaba Cloud). These models have revolutionized natural language processing by enabling more sophisticated interactions between humans and machines.\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04134b94-6345-4d1b-9175-18109ce93b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67334f1f-e2f6-429a-a40c-30d655c440a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: system\n",
      "You are a helpful AI assistant.\n",
      "user\n",
      "hi\n",
      "assistant\n",
      "Hello! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Translate this English sentence into Spanish: 'Technology is shaping the future.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: system\n",
      "You are a helpful AI assistant.\n",
      "user\n",
      "Translate this English sentence into Spanish: 'Technology is shaping the future.'\n",
      "assistant\n",
      "La tecnología está moldeando el futuro.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Write a poem on spring season\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: system\n",
      "You are a helpful AI assistant.\n",
      "user\n",
      "Write a poem on spring season\n",
      "assistant\n",
      "Spring, the season of new birth,  \n",
      "Whispers through the budding earth.  \n",
      "The frost retreats with every breeze,  \n",
      "And life awakes from winter's freeze.  \n",
      "\n",
      "The trees, once bare and cold, now wear  \n",
      "A crown of green that softens air.  \n",
      "Buds peek out, in hues of pink,  \n",
      "Like nature's own, sweet, secret drink.  \n",
      "\n",
      "The birds return to skies so blue,  \n",
      "Singing songs of joy anew.  \n",
      "Their melodies, a lively tune,  \n",
      "Echo through the waking town.  \n",
      "\n",
      "The rivers hum a gentle hum,  \n",
      "Reflecting clouds in silver bloom.  \n",
      "Each drop a mirror, pure and clear,  \n",
      "Of skies and fields, both wild and near.  \n",
      "\n",
      "The flowers bloom in colors bright,  \n",
      "A canvas painted with delight.  \n",
      "Reds, pinks, yellows, violets too,  \n",
      "A tapestry for nature's view.  \n",
      "\n",
      "The sun, more warm than cold before,  \n",
      "Kisses each blade of fresh green grass,  \n",
      "As if to say, \"Awake, arise,  \n",
      "Let life be what you choose to prize.\"  \n",
      "\n",
      "Oh, spring, you bring us hope and cheer,  \n",
      "A time when all is fresh and clear.  \n",
      "In every breath, in every sight,  \n",
      "You teach us how to live just right.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Summarise this content: Introduction Two months after upgrading Qwen2.5-Turbo to support context length up to one million tokens, we are back with the open-source Qwen2.5-1M models and the corresponding inference framework support. Here’s what you can expect from this release:  Opensource Models: We’re releasing two new checkpoints, Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, marking the first time we’ve upgraded our opensource Qwen models to handle 1M-token contexts.  Inference Framework: To help developers deploy the Qwen2.5-1M series models more efficiently, we’ve fully open-sourced our inference framework based on vLLM. With integration with sparse attention methods, our framework can process 1M-token inputs 3x to 7x faster.  Technical Report: We’re also sharing the technical details behind the Qwen2.5-1M series, including design insights for training and inference frameworks, as well as ablation experiments.  You can experience Qwen2.5-1M models online by visiting our demo on Huggingface and Modelscope.  Additionally, we recently introduced Qwen Chat, an advanced AI assistant from the Qwen series. With Qwen Chat, you can engage in conversations, write code, perform searches, generate images and videos, and utilize various tools. Notably, Qwen Chat also features the Qwen2.5-Turbo model, which supports long-context processing with a context length of up to 1M tokens.  Model Performance Let’s start by diving into the performance of the Qwen2.5-1M series models, covering both long-context and short text tasks.  Long-Context Tasks First off, we evaluate the Qwen2.5-1M models on the Passkey Retrieval task with a context length of 1 million tokens. The results show that these models can accurately retrieve hidden information from documents containing up to 1M tokens, with only minor errors observed in the 7B model.   For more complex long-context understanding tasks, we select RULER, LV-Eval, LongbenchChat used in this blog.    From these results, we can draw a few key conclusions:  Significantly Superior to the 128k Version: The Qwen2.5-1M series models significantly outperform their 128K counterparts in most long-context tasks, especially for sequences exceeding 64K in length. Notable Performance Advantage: The Qwen2.5-14B-Instruct-1M model not only beats Qwen2.5-Turbo but also consistently outperforms GPT-4o-mini across multiple datasets, offering a robust open-source alternative for long-context tasks. Short-Context Tasks Besides performance on long sequences, we’re equally interested in how these models handle short sequences. So, we compare the Qwen2.5-1M models and their 128K versions on widely used academic benchmarks, throwing in GPT-4o-mini for comparison.   Here’s what we find:  Both Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M maintain performance on short text tasks that is similar to their 128K versions, ensuring the fundamental capabilities haven’t been compromised by the addition of long-sequence processing abilities. Compared to GPT-4o-mini, both Qwen2.5-14B-Instruct-1M and Qwen2.5-Turbo achieve similar performance on short text tasks while supporting a context length that’s eight times longer. Key Techniques Here, we’ll briefly introduce the key techniques behind building Qwen2.5-1M. For more details, please check out our technical report.  Long-Context Training  Training with long sequences demands substantial computational resources, so we adopt a progressive approach to expand the context length for Qwen2.5-1M through multiple stages:  We begin with an intermediate checkpoint of pre-trained Qwen2.5, which had a 4K token context length. In Pretraining, we gradually increase the context length from 4K to 256K tokens while using Adjusted Base Frequency, raising the RoPE base from 10,000 to 10,000,000. In Supervised Fine-tuning, we split this into two stages to preserve performance on shorter sequences: Stage 1: Fine-tuned only on short instructions (up to 32K tokens) using the same data and steps as the 128K versions of Qwen2.5. Stage 2: Mixed short (up to 32K) and long (up to 256K) instructions to enhance long-context task performance while maintaining short-task quality. In Reinforcement Learning, we train models on short texts up to 8K tokens, which sufficiently improves alignment with human preferences and generalizes well to long-context tasks. The final instruction-tuned models are capable of handling sequences up to 256K tokens.  Length Extrapolation During training, we develop an instruction-tuned model with a context length of 256K tokens. To extend this to 1M tokens, we employ length extrapolation techniques.  The degradation of LLMs based on RoPE in long-context tasks is mainly due to unseen, large relative positional distances between queries and keys in computing attention weight. We employ Dual Chunk Attention (DCA), which addresses this issue by remapping relative positions to smaller values, avoiding the large distances not seen during training.   We evaluat the Qwen2.5-1M models and their 128K counterparts with and without the length extrapolation method. We can find:  Even models trained on just 32K tokens, such as the Qwen2.5-7B-Instruct, achieve nearly perfect accuracy in passkey retrieval tasks with 1M-token contexts. This underscores the remarkable ability of DCA to extend supported context lengths, without any training required.   Sparse Attention For long-context language models, inference speed is crucial for user experience. We introduce a sparse attention mechanism based on MInference to accelerate the prefill phase. Furthermore, we propose several improvements:  Integrating with Chunked Prefill: Directly processing sequences of 1M tokens results in substantial memory overhead to store the activations in MLP layers, consuming 71GB of VRAM in Qwen2.5-7B. By integrating with chunk prefill with a chunk length of 32,768 tokens, activation VRAM usage is reduced by 96.7%, leading to a significant decrease in memory consumption.  Integrating with Length Extrapolation: We integrate DCA with MInference in long-context processing, thereby enhancing inference efficiency and achieving greater accuracy.  Sparsity Refinement on Long Sequences: MInference requires an offline search to determine the optimal sparsification configuration for each attention head. Due to the computational demand of full attention weights, this search is typically conducted on short sequences, which may not generalize well to longer sequences. We developed a method to refine the sparsification configuration specifically for sequences up to 1M tokens, which significantly reduces the accuracy loss brought by sparse attention.  More Optimizations: We introduce additional optimizations, such as enhanced kernel efficiency and dynamic chunked pipeline parallelism, to fully unlock the potential of the entire framework.  With these enhancements, our inference framework results in a 3.2x to 6.7x acceleration in the prefill speed across different model sizes and GPU devices for sequences of 1M token length.   Deploy Qwen2.5-1M Models Locally Here we provide step-by-step instructions for deploying the Qwen2.5-1M models on your local devices.  1. System Preparation To achieve the best performance, we recommend using GPUs with Ampere or Hopper architecture, which support optimized kernels.  Ensure your system meets the following requirements:  CUDA Version: 12.1 or 12.3 Python Version: >=3.9 and <=3.12 VRAM Requirement for processing 1 million-token sequences:  Qwen2.5-7B-Instruct-1M: At least 120GB VRAM (total across GPUs). Qwen2.5-14B-Instruct-1M: At least 320GB VRAM (total across GPUs). If your GPUs do not have sufficient VRAM, you can still use Qwen2.5-1M models for shorter tasks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: system\n",
      "You are a helpful AI assistant.\n",
      "user\n",
      "Summarise this content: Introduction Two months after upgrading Qwen2.5-Turbo to support context length up to one million tokens, we are back with the open-source Qwen2.5-1M models and the corresponding inference framework support. Here’s what you can expect from this release:  Opensource Models: We’re releasing two new checkpoints, Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, marking the first time we’ve upgraded our opensource Qwen models to handle 1M-token contexts.  Inference Framework: To help developers deploy the Qwen2.5-1M series models more efficiently, we’ve fully open-sourced our inference framework based on vLLM. With integration with sparse attention methods, our framework can process 1M-token inputs 3x to 7x faster.  Technical Report: We’re also sharing the technical details behind the Qwen2.5-1M series, including design insights for training and inference frameworks, as well as ablation experiments.  You can experience Qwen2.5-1M models online by visiting our demo on Huggingface and Modelscope.  Additionally, we recently introduced Qwen Chat, an advanced AI assistant from the Qwen series. With Qwen Chat, you can engage in conversations, write code, perform searches, generate images and videos, and utilize various tools. Notably, Qwen Chat also features the Qwen2.5-Turbo model, which supports long-context processing with a context length of up to 1M tokens.  Model Performance Let’s start by diving into the performance of the Qwen2.5-1M series models, covering both long-context and short text tasks.  Long-Context Tasks First off, we evaluate the Qwen2.5-1M models on the Passkey Retrieval task with a context length of 1 million tokens. The results show that these models can accurately retrieve hidden information from documents containing up to 1M tokens, with only minor errors observed in the 7B model.   For more complex long-context understanding tasks, we select RULER, LV-Eval, LongbenchChat used in this blog.    From these results, we can draw a few key conclusions:  Significantly Superior to the 128k Version: The Qwen2.5-1M series models significantly outperform their 128K counterparts in most long-context tasks, especially for sequences exceeding 64K in length. Notable Performance Advantage: The Qwen2.5-14B-Instruct-1M model not only beats Qwen2.5-Turbo but also consistently outperforms GPT-4o-mini across multiple datasets, offering a robust open-source alternative for long-context tasks. Short-Context Tasks Besides performance on long sequences, we’re equally interested in how these models handle short sequences. So, we compare the Qwen2.5-1M models and their 128K versions on widely used academic benchmarks, throwing in GPT-4o-mini for comparison.   Here’s what we find:  Both Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M maintain performance on short text tasks that is similar to their 128K versions, ensuring the fundamental capabilities haven’t been compromised by the addition of long-sequence processing abilities. Compared to GPT-4o-mini, both Qwen2.5-14B-Instruct-1M and Qwen2.5-Turbo achieve similar performance on short text tasks while supporting a context length that’s eight times longer. Key Techniques Here, we’ll briefly introduce the key techniques behind building Qwen2.5-1M. For more details, please check out our technical report.  Long-Context Training  Training with long sequences demands substantial computational resources, so we adopt a progressive approach to expand the context length for Qwen2.5-1M through multiple stages:  We begin with an intermediate checkpoint of pre-trained Qwen2.5, which had a 4K token context length. In Pretraining, we gradually increase the context length from 4K to 256K tokens while using Adjusted Base Frequency, raising the RoPE base from 10,000 to 10,000,000. In Supervised Fine-tuning, we split this into two stages to preserve performance on shorter sequences: Stage 1: Fine-tuned only on short instructions (up to 32K tokens) using the same data and steps as the 128K versions of Qwen2.5. Stage 2: Mixed short (up to 32K) and long (up to 256K) instructions to enhance long-context task performance while maintaining short-task quality. In Reinforcement Learning, we train models on short texts up to 8K tokens, which sufficiently improves alignment with human preferences and generalizes well to long-context tasks. The final instruction-tuned models are capable of handling sequences up to 256K tokens.  Length Extrapolation During training, we develop an instruction-tuned model with a context length of 256K tokens. To extend this to 1M tokens, we employ length extrapolation techniques.  The degradation of LLMs based on RoPE in long-context tasks is mainly due to unseen, large relative positional distances between queries and keys in computing attention weight. We employ Dual Chunk Attention (DCA), which addresses this issue by remapping relative positions to smaller values, avoiding the large distances not seen during training.   We evaluat the Qwen2.5-1M models and their 128K counterparts with and without the length extrapolation method. We can find:  Even models trained on just 32K tokens, such as the Qwen2.5-7B-Instruct, achieve nearly perfect accuracy in passkey retrieval tasks with 1M-token contexts. This underscores the remarkable ability of DCA to extend supported context lengths, without any training required.   Sparse Attention For long-context language models, inference speed is crucial for user experience. We introduce a sparse attention mechanism based on MInference to accelerate the prefill phase. Furthermore, we propose several improvements:  Integrating with Chunked Prefill: Directly processing sequences of 1M tokens results in substantial memory overhead to store the activations in MLP layers, consuming 71GB of VRAM in Qwen2.5-7B. By integrating with chunk prefill with a chunk length of 32,768 tokens, activation VRAM usage is reduced by 96.7%, leading to a significant decrease in memory consumption.  Integrating with Length Extrapolation: We integrate DCA with MInference in long-context processing, thereby enhancing inference efficiency and achieving greater accuracy.  Sparsity Refinement on Long Sequences: MInference requires an offline search to determine the optimal sparsification configuration for each attention head. Due to the computational demand of full attention weights, this search is typically conducted on short sequences, which may not generalize well to longer sequences. We developed a method to refine the sparsification configuration specifically for sequences up to 1M tokens, which significantly reduces the accuracy loss brought by sparse attention.  More Optimizations: We introduce additional optimizations, such as enhanced kernel efficiency and dynamic chunked pipeline parallelism, to fully unlock the potential of the entire framework.  With these enhancements, our inference framework results in a 3.2x to 6.7x acceleration in the prefill speed across different model sizes and GPU devices for sequences of 1M token length.   Deploy Qwen2.5-1M Models Locally Here we provide step-by-step instructions for deploying the Qwen2.5-1M models on your local devices.  1. System Preparation To achieve the best performance, we recommend using GPUs with Ampere or Hopper architecture, which support optimized kernels.  Ensure your system meets the following requirements:  CUDA Version: 12.1 or 12.3 Python Version: >=3.9 and <=3.12 VRAM Requirement for processing 1 million-token sequences:  Qwen2.5-7B-Instruct-1M: At least 120GB VRAM (total across GPUs). Qwen2.5-14B-Instruct-1M: At least 320GB VRAM (total across GPUs). If your GPUs do not have sufficient VRAM, you can still use Qwen2.5-1M models for shorter tasks.\n",
      "assistant\n",
      "### Summary of Qwen2.5-1M Release\n",
      "\n",
      "**Introduction:**\n",
      "Two months after upgrading Qwen2.5-Turbo to support one million token contexts, the Qwen2.5-1M models and their inference framework are now open-sourced. This update includes:\n",
      "- Two new checkpoints: Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M.\n",
      "- An open-sourced inference framework based on vLLM, integrating sparse attention methods for faster processing.\n",
      "\n",
      "**Performance:**\n",
      "- **Long-Context Tasks:** The models excel at tasks requiring up to 1 million token contexts, like document retrieval, showing minimal errors in the 7B model. The Qwen2.5-14B-Instruct-1M outperforms both Qwen2.5-Turbo and GPT-4o-mini across multiple datasets.\n",
      "- **Short-Context Tasks:** The models maintain comparable performance to their 128K versions, ensuring no degradation in short text tasks despite added long-sequence capabilities.\n",
      "\n",
      "**Key Techniques:**\n",
      "- **Long-Context Training:** A progressive approach was used, starting from a 4K token context and gradually increasing it to 256K tokens, with techniques like Adjusted Base Frequency and Dual Chunk Attention (DCA).\n",
      "- **Length Extrapolation:** DCA was employed to extend the context length from 256K to 1M tokens, addressing the issue of large relative positional distances.\n",
      "- **Sparse Attention:** Improvements include chunked prefill, integration with length extrapolation, and refined sparsification configurations for long sequences, resulting in up to 6.7x faster prefill speeds.\n",
      "\n",
      "**Deployment:**\n",
      "For local deployment:\n",
      "- Use GPUs with Ampere or Hopper architecture.\n",
      "- Ensure the system meets specific VRAM requirements: at least 120GB for Qwen2.5-7B-Instruct-1M and 320GB for Qwen2.5-14B-Instruct-1M.\n",
      "\n",
      "**Additional Features:**\n",
      "Qwen Chat, an advanced AI assistant from the Qwen series, incorporates the Qwen2.5-Turbo model, supporting long-context processing with a context length of up to 1M tokens.\n",
      "\n",
      "This release provides significant advancements in handling long-context tasks while maintaining performance on short tasks, along with detailed technical insights shared in the technical report.\n"
     ]
    }
   ],
   "source": [
    "# # Chat Loop\n",
    "while True:\n",
    "    user_input = input(\"User: \")  # Get user input\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Format the input for the model\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "\n",
    "    # Tokenize and process input\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=512)\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Display response\n",
    "    print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267fa62-e15d-4ea6-ad36-c56f845f0f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b88b1-dff3-4ec8-baad-c56886e0743a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
